{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Firstly, I want to thank H&M for hosting this amazing competition and everyone who shares his/her ideas/code during the competition which helps me, a rookie in the recommending field, a lot to learn some practical skills.\n\nSecondly, I want to thank my teammates [@tarick.morty](https://www.kaggle.com/tarique7) [@yuanzhe zhou](https://www.kaggle.com/yuanzhezhou) [@YijunYANG123](https://www.kaggle.com/yijunyang123) [@Chenm](https://www.kaggle.com/chenmientan) who contribute a lot to make our final rank possible.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://github.com/Wp-Zhang/H-M-Fashion-RecSys/blob/main/imgs/img2.png?raw=true\">\n\nOur team ranked 45/3006 in the end with a LB score of 0.0292 and a PB score of 0.02996. \n\n**Our final solution contains 2 recall strategies and we trained 3 different ranking models (LGB ranker, LGB classifier, DNN) for each strategy.**\n\nCandidates from the two strategies are quite different so that ensembling the ranking results can help to improve the score. From our experiments, LB score of a single recall strategy can only reach 0.0286 and ensembling helps us to boost up to 0.0292. We also believe that ensembling can make our predicting result more robust.\n\nDue to hardware limits (50G of RAM), we only generated avg **50 candidates** for each user and used **4 weeks** of data to train the models.","metadata":{}},{"cell_type":"markdown","source":"This notebook contains the code of our first recall strategy, feature engineering, and training a LGB ranker/classifier. Predicting reasult from a single LGB ranker in this notebok can reach a LB score of 0.0276.\n \n **For full version of our solution, please check out our [GitHub repo](https://github.com/Wp-Zhang/H-M-Fashion-RecSys) where all code is well structured and documented ðŸ˜€.**","metadata":{}},{"cell_type":"markdown","source":"> BTW, I'm a graduate student who is seeking a Data Scientist/MLE internship starting in 2022 fall/2023 sping and a full-time position in 2023 summer. If you are hiring or you are willing to provide a reference, please contact me through [zhang.weipe@northeastern.edu](zhang.weipe@northeastern.edu) ðŸ˜€, thanks!","metadata":{}},{"cell_type":"markdown","source":"Note: This notebook is just for sharing our solution and cannot run directly on kaggle.","metadata":{}},{"cell_type":"code","source":"%pip install -U lightgbm==3.3.2\n%pip install implicit","metadata":{"id":"Ej-kIjP5d_zw","outputId":"5a6da2ba-33b9-4c64-c3e9-095d8506a867"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"Iq3oG1iwJan4","outputId":"5f7e9ba8-2e7d-4246-f23b-b2575af3a9d6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom pandas.api.types import CategoricalDtype\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\n\nimport pickle\nfrom tqdm import tqdm\nimport gc\nfrom pathlib import Path","metadata":{"id":"gboISq82XUN1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nimport sys\nfrom IPython.core.interactiveshell import InteractiveShell\n\nwarnings.filterwarnings(\"ignore\")\nsys.path.append(\"/content/drive/MyDrive/HM/\") # path to the `src`` folder\nInteractiveShell.ast_node_interactivity = \"all\"\ntqdm.pandas()","metadata":{"id":"F6WO2VtEXefj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# * Note: the src module can be found at https://github.com/Wp-Zhang/H-M-Fashion-RecSys/tree/main/src.\nfrom src.data import DataHelper\nfrom src.data.metrics import map_at_k, hr_at_k, recall_at_k\n\nfrom src.retrieval.rules import (\n    OrderHistory,\n    OrderHistoryDecay,\n    ItemPair,\n    UserGroupTimeHistory,\n    UserGroupSaleTrend,\n    TimeHistory,\n    TimeHistoryDecay,\n    SaleTrend,\n    OutOfStock,\n)\nfrom src.retrieval.collector import RuleCollector\n\nfrom src.features import full_sale, week_sale, repurchase_ratio, popularity, period_sale\n\nfrom src.utils import (\n    calc_valid_date,\n    merge_week_data,\n    reduce_mem_usage,\n    calc_embd_similarity,\n)","metadata":{"id":"zxhDyWVbJR3w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = Path(\"/content/drive/MyDrive/HM/data/\")\nmodel_dir = Path(\"/content/drive/MyDrive/HM/models/\")","metadata":{"id":"lL6IRSJ2JR3x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_WEEK_NUM = 4\nWEEK_NUM = TRAIN_WEEK_NUM + 2\n\nVERSION_NAME = \"Recall 1\"\nTEST = True # * Set as `False` when do local experiments to save time","metadata":{"id":"AakTfKUQJR3x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nif not os.path.exists(data_dir/\"interim\"/VERSION_NAME):\n    os.mkdir(data_dir/\"interim\"/VERSION_NAME)\nif not os.path.exists(data_dir/\"processed\"/VERSION_NAME):\n    os.mkdir(data_dir/\"processed\"/VERSION_NAME)","metadata":{"id":"RHu8rsZMJR3y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pepare data: encoding ids and preprocessing","metadata":{"id":"Mazy6bP0JR30"}},{"cell_type":"code","source":"dh = DataHelper(data_dir)","metadata":{"id":"2BPv2lcWGGQJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = dh.preprocess_data(save=True, name=\"encoded_full\") # * run only once, processed data will be saved","metadata":{"id":"kiIB1uvjJR33"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = dh.load_data(name=\"encoded_full\")","metadata":{"id":"i3ngtb7NUGbH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uid2idx = pickle.load(open(data_dir/\"index_id_map/user_id2index.pkl\", \"rb\"))\nsubmission = pd.read_csv(data_dir/\"raw\"/'sample_submission.csv')\nsubmission['customer_id'] = submission['customer_id'].map(uid2idx)","metadata":{"id":"7ecaPObIJR34"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Retrieval\n","metadata":{"id":"LX4M190s4pxo"}},{"cell_type":"markdown","source":"Generate candidates for each week","metadata":{"id":"fcShMOQPJR35"}},{"cell_type":"code","source":"listBin = [-1, 19, 29, 39, 49, 59, 69, 119]\ndata['user']['age_bins'] = pd.cut(data['user']['age'], listBin)","metadata":{"id":"J7gu-mwjJR36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# * WEEK_NUM = 0: test\n# * WEEK_NUM = 1: valid\n# * WEEK_NUM > 1: train\nfor week in range(1,WEEK_NUM):\n    # * use sliding window to generate candidates\n    if week == 0 and not TEST:\n        continue\n    trans = data[\"inter\"]\n\n    start_date, end_date = calc_valid_date(week)\n    print(f\"Week {week}: [{start_date}, {end_date})\")\n    \n    train, valid = dh.split_data(trans, start_date, end_date)\n    train = train.merge(data['user'][['customer_id','age_bins']], on='customer_id', how='left')\n\n    last_week_start = pd.to_datetime(start_date) - pd.Timedelta(days=7)\n    last_week_start = last_week_start.strftime(\"%Y-%m-%d\")\n    last_week = train.loc[train.t_dat >= last_week_start]\n    \n    last_3day_start = pd.to_datetime(start_date) - pd.Timedelta(days=3)\n    last_3day_start = last_3day_start.strftime(\"%Y-%m-%d\")\n    last_3days = train.loc[train.t_dat >= last_3day_start]\n\n    if week != 0:\n        customer_list = valid[\"customer_id\"].values\n    else:\n        customer_list = submission['customer_id'].values\n\n    # * ========================== Retrieval Strategies ==========================\n\n    candidates = RuleCollector().collect(\n        week_num = week,\n        trans_df = trans,\n        customer_list=customer_list,\n        rules=[\n            OrderHistory(train, days=3, name='1'),\n            OrderHistory(train, days=7, name='2'),\n            OrderHistoryDecay(train, days=3, n=50, name='1'),\n            OrderHistoryDecay(train, days=7, n=50, name='2'),\n            ItemPair(OrderHistory(train, days=3).retrieve(), name='1'),\n            ItemPair(OrderHistory(train, days=7).retrieve(), name='2'),\n            ItemPair(OrderHistoryDecay(train, days=3, n=50).retrieve(), name='3'),\n            ItemPair(OrderHistoryDecay(train, days=7, n=50).retrieve(), name='4'),\n            UserGroupTimeHistory(data, customer_list, last_week, ['age_bins'], n=50, name='1'),\n            UserGroupTimeHistory(data, customer_list, last_3days, ['age_bins'], n=50, name='2'),\n            UserGroupSaleTrend(data, customer_list, train, ['age_bins'], days=7, n=50),\n            TimeHistory(customer_list, last_week, n=50, name='1'),\n            TimeHistory(customer_list, last_3days, n=50, name='2'),\n            TimeHistoryDecay(customer_list, train, days=3, n=50, name='1'),\n            TimeHistoryDecay(customer_list, train, days=7, n=50, name='2'),\n            SaleTrend(customer_list, train, days=7, n=50),\n        ],\n        filters=[OutOfStock(trans)],\n        min_pos_rate=0.006,\n        compress=False,\n    )\n\n    candidates = (\n        pd.pivot_table(\n            candidates,\n            values=\"score\",\n            index=[\"customer_id\", \"article_id\"],\n            columns=[\"method\"],\n            aggfunc=np.sum,\n        )\n        .reset_index()\n    )\n\n    candidates.to_parquet(data_dir/\"interim\"/VERSION_NAME/f\"week{week}_candidate.pqt\")\n    valid.to_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week{week}_label.pqt\")","metadata":{"id":"glOiftg3JR36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# * use the threshold in week 1 to generate candidates for test data, see the log in the upper cell \nif TEST:\n    week = 0\n    trans = data[\"inter\"]\n    \n    start_date, end_date = calc_valid_date(week)\n    print(f\"Week {week}: [{start_date}, {end_date})\")\n    \n    train, valid = dh.split_data(trans, start_date, end_date)\n    train = train.merge(data['user'][['customer_id','age_bins']], on='customer_id', how='left')\n\n    last_week_start = pd.to_datetime(start_date) - pd.Timedelta(days=7)\n    last_week_start = last_week_start.strftime(\"%Y-%m-%d\")\n    last_week = train.loc[train.t_dat >= last_week_start]\n    \n    last_3day_start = pd.to_datetime(start_date) - pd.Timedelta(days=3)\n    last_3day_start = last_3day_start.strftime(\"%Y-%m-%d\")\n    last_3days = train.loc[train.t_dat >= last_3day_start]\n\n    customer_list = submission['customer_id'].values\n\n    # * ========================== Retrieval Strategies ==========================\n\n    candidates = RuleCollector().collect(\n        week_num = week,\n        trans_df = trans,\n        customer_list=customer_list,\n        rules=[\n            OrderHistory(train, days=3, name='1'),\n            OrderHistory(train, days=7, name='2'),\n            OrderHistoryDecay(train, days=3, n=50, name='1'),\n            OrderHistoryDecay(train, days=7, n=50, name='2'),\n            ItemPair(OrderHistory(train, days=3).retrieve(), name='1'),\n            ItemPair(OrderHistory(train, days=7).retrieve(), name='2'),\n            ItemPair(OrderHistoryDecay(train, 3, n=50).retrieve(), name='3'),\n            ItemPair(OrderHistoryDecay(train, 7, n=50).retrieve(), name='4'),\n            UserGroupTimeHistory(data, customer_list, last_week, ['age_bins'], n=15, name='1'),\n            UserGroupTimeHistory(data, customer_list, last_3days, ['age_bins'], n=20.5, name='2'),\n            UserGroupSaleTrend(data, customer_list, train, ['age_bins'], days=7, n=2),\n            TimeHistory(customer_list, last_week, n=9, name='1'),\n            TimeHistory(customer_list, last_3days, n=16, name='2'),\n            TimeHistoryDecay(customer_list, train, days=3, n=12, name='1'),\n            TimeHistoryDecay(customer_list, train, days=7, n=8, name='2'),\n            SaleTrend(customer_list, train, days=7, n=2),\n        ],\n        filters=[OutOfStock(trans)],\n        min_pos_rate=0.006,\n        compress=False,\n    )\n    \n    candidates, _ = reduce_mem_usage(candidates)\n    candidates = (\n        pd.pivot_table(\n            candidates,\n            values=\"score\",\n            index=[\"customer_id\", \"article_id\"],\n            columns=[\"method\"],\n            aggfunc=np.sum,\n        )\n        .reset_index()\n    )\n\n    candidates.to_parquet(data_dir/\"interim\"/VERSION_NAME/f\"week{week}_candidate.pqt\")\n    valid.to_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week{week}_label.pqt\")","metadata":{"id":"3wqDho1ZJR38"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train, valid, last_week, customer_list, candidates\ngc.collect()","metadata":{"id":"gQEog9JJJR39"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature engineering\n","metadata":{"id":"TBIrKxhRGGQR"}},{"cell_type":"code","source":"user = data[\"user\"]\nitem = data[\"item\"]\ninter = data[\"inter\"]","metadata":{"id":"9pF-sOMSGGQP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate week number\ninter['week'] = (pd.to_datetime('2020-09-29') - pd.to_datetime(inter['t_dat'])).dt.days // 7","metadata":{"id":"heKIAtoRdtIk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge full candidates to transaction data (avoid feature missing in training data)\nfull_candidates = []\nfor i in tqdm(range(WEEK_NUM)):\n    candidate = pd.read_parquet(data_dir/\"interim\"/VERSION_NAME/f\"week{i}_candidate.pqt\")\n    full_candidates += candidate['article_id'].values.tolist()\nfull_candidates = list(set(full_candidates))\ndel candidate\ngc.collect()\n\nnum_candidates = len(full_candidates)\nfull_candidates = np.array(full_candidates)\nfull_candidates = np.tile(full_candidates, WEEK_NUM + 1)\nweeks = np.repeat(np.arange(1,WEEK_NUM+2), num_candidates)\nfull_candidates = pd.DataFrame({'article_id':full_candidates, 'week':weeks})\n\ninter['valid'] = 1\nin_train = inter[inter['week']<=WEEK_NUM + 1]\nout_train = inter[inter['week']>WEEK_NUM + 1]\n\nin_train = in_train.merge(full_candidates, on=['article_id','week'], how='right')\nin_train['valid'] = in_train['valid'].fillna(0)\ninter = pd.concat([in_train, out_train], ignore_index=True)\ninter = inter.sort_values([\"valid\"], ascending=False).reset_index(drop=True)","metadata":{"id":"kaEwbD6KY5R8","outputId":"f71ff8a2-e5e1-4aa8-b1ce-91462fcaa87c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge `product_code`\ninter = inter.merge(item[[\"article_id\", \"product_code\"]], on=\"article_id\", how=\"left\")","metadata":{"id":"QsW8gPPUUUg_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inter.shape","metadata":{"id":"Hz4m4zsiieqn","outputId":"f4855f51-d2ea-4c3b-f1f2-a59ecbce5000"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, inter[\"i_1w_sale_rank\"], inter[\"i_1w_sale_norm\"] = period_sale(\n    inter, [\"article_id\"], days=14, rank=True, norm=True, week_num=WEEK_NUM\n)\n_, inter[\"p_1w_sale_rank\"], inter[\"p_1w_sale_norm\"] = period_sale(\n    inter, [\"product_code\"], days=14, rank=True, norm=True, week_num=WEEK_NUM\n)\ninter[\"i_2w_sale\"], inter[\"i_2w_sale_rank\"], inter[\"i_2w_sale_norm\"] = period_sale(\n    inter, [\"article_id\"], days=14, rank=True, norm=True, week_num=WEEK_NUM\n)\ninter[\"p_2w_sale\"], inter[\"p_2w_sale_rank\"], inter[\"p_2w_sale_norm\"] = period_sale(\n    inter, [\"product_code\"], days=14, rank=True, norm=True, week_num=WEEK_NUM\n)","metadata":{"id":"kYdOqlyldvNM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inter[\"i_3w_sale\"], inter[\"i_3w_sale_rank\"], inter[\"i_3w_sale_norm\"] = period_sale(\n    inter, [\"article_id\"], days=21, rank=True, norm=True, week_num=WEEK_NUM\n)\ninter[\"p_3w_sale\"], inter[\"p_3w_sale_rank\"], inter[\"p_3w_sale_norm\"] = period_sale(\n    inter, [\"product_code\"], days=21, rank=True, norm=True, week_num=WEEK_NUM\n)\ninter[\"i_4w_sale\"], inter[\"i_4w_sale_rank\"], inter[\"i_4w_sale_norm\"] = period_sale(\n    inter, [\"article_id\"], days=28, rank=True, norm=True, week_num=WEEK_NUM\n)\ninter[\"p_4w_sale\"], inter[\"p_4w_sale_rank\"], inter[\"p_4w_sale_norm\"] = period_sale(\n    inter, [\"product_code\"], days=28, rank=True, norm=True, week_num=WEEK_NUM\n)","metadata":{"id":"7Lv1FtoOeI9n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inter.shape","metadata":{"id":"2Yqb3KaWinZM","outputId":"06dc54f9-2c38-4fa5-f242-f8d42ad7a5cc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inter['i_repurchase_ratio'] = repurchase_ratio(inter, ['article_id'], week_num=WEEK_NUM)\ninter['p_repurchase_ratio'] = repurchase_ratio(inter, ['product_code'], week_num=WEEK_NUM)","metadata":{"id":"FdlZqq5QzbMR","outputId":"dab8fbf6-0167-42d6-9e74-ecc826ec84c1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inter.shape","metadata":{"id":"Oh3Otv_Cipmi","outputId":"0224c3f9-8534-4658-edca-2f0dc887f728"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inter, _ = reduce_mem_usage(inter)","metadata":{"id":"riX-F5H5lbMu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inter[\"i_sale\"] = week_sale(inter, [\"article_id\"], week_num=WEEK_NUM)\ninter[\"p_sale\"] = week_sale(inter, [\"product_code\"], week_num=WEEK_NUM)\ninter[\"i_sale_uni\"] = week_sale(inter, [\"article_id\"], True, week_num=WEEK_NUM)\ninter[\"p_sale_uni\"] = week_sale(inter, [\"product_code\"], True, week_num=WEEK_NUM)\ninter[\"lw_i_sale\"] = week_sale(inter, [\"article_id\"], step=1, week_num=WEEK_NUM) # * last week sale\ninter[\"lw_p_sale\"] = week_sale(inter, [\"product_code\"], step=1, week_num=WEEK_NUM)\ninter[\"lw_i_sale_uni\"] = week_sale(inter, [\"article_id\"], True, step=1, week_num=WEEK_NUM)\ninter[\"lw_p_sale_uni\"] = week_sale(inter, [\"product_code\"], True, step=1, week_num=WEEK_NUM)\n\ninter[\"i_sale_ratio\"] = inter[\"i_sale\"] / (inter[\"p_sale\"] + 1e-6)\ninter[\"i_sale_uni_ratio\"] = inter[\"i_sale_uni\"] / (inter[\"p_sale_uni\"] + 1e-6)\ninter[\"lw_i_sale_ratio\"] = inter[\"lw_i_sale\"] / (inter[\"lw_p_sale\"] + 1e-6)\ninter[\"lw_i_sale_uni_ratio\"] = inter[\"lw_i_sale_uni\"] / (inter[\"lw_p_sale_uni\"] + 1e-6)\n\ninter[\"i_uni_ratio\"] = inter[\"i_sale\"] / (inter[\"i_sale_uni\"] + 1e-6)\ninter[\"p_uni_ratio\"] = inter[\"p_sale\"] / (inter[\"p_sale_uni\"] + 1e-6)\ninter[\"lw_i_uni_ratio\"] = inter[\"lw_i_sale\"] / (inter[\"lw_i_sale_uni\"] + 1e-6)\ninter[\"lw_p_uni_ratio\"] = inter[\"lw_p_sale\"] / (inter[\"lw_p_sale_uni\"] + 1e-6)\n\ninter[\"i_sale_trend\"] = (inter[\"i_sale\"] - inter[\"lw_i_sale\"]) / (inter[\"lw_i_sale\"] + 1e-6)\ninter[\"p_sale_trend\"] = (inter[\"p_sale\"] - inter[\"lw_p_sale\"]) / (inter[\"lw_p_sale\"] + 1e-6)\n\nitem_feats = [\n    \"product_type_no\",\n    # \"product_group_name\",\n    # \"graphical_appearance_no\",\n    # \"colour_group_code\",\n    # \"perceived_colour_value_id\",\n    # \"perceived_colour_master_id\",\n]\ninter = inter.merge(item[[\"article_id\", *item_feats]], on=\"article_id\", how=\"left\")\n\nfor f in tqdm(item_feats):\n    inter[f\"{f}_sale\"] = week_sale(inter, [f], f\"{f}_sale\", week_num=WEEK_NUM)\n    inter[f\"lw_{f}_sale\"] = week_sale(inter, [f], f\"{f}_sale\", step=1, week_num=WEEK_NUM)\n    inter[f\"{f}_sale_trend\"] = (inter[f\"{f}_sale\"] - inter[f\"lw_{f}_sale\"]) / (inter[f\"lw_{f}_sale\"] + 1e-6)","metadata":{"id":"70okKvWfzgip","outputId":"a1a2392a-c88d-49f3-b4be-d3aebecb7761"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inter.shape","metadata":{"id":"K4hO0oT3isNu","outputId":"bf229999-43ac-4e2d-ee40-eb93d608911e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# * Date related\ncurr_date_dict = {x:calc_valid_date(x-1)[0] for x in range(100)}\ncurrent_dat = inter['week'].map(curr_date_dict)\nmask = inter['valid']==0\ninter.loc[mask, 't_dat'] = inter.loc[mask, 'week'].map(curr_date_dict)\nfirst_date = inter.groupby('article_id')['t_dat'].min().reset_index(name='first_dat')\ninter = pd.merge(inter, first_date, on='article_id', how='left')\n# df = pd.merge(df, last_date, on='article_id', how='left')\ninter['first_dat'] = (pd.to_datetime(current_dat)-pd.to_datetime(inter['first_dat'])).dt.days","metadata":{"id":"mQkGNyjaBbi-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inter.shape","metadata":{"id":"sZHdSXtAi_zK","outputId":"2a802998-74c8-4695-95fd-d134d3d7d549"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inter['i_full_sale'] = full_sale(inter, ['article_id'], week_num=WEEK_NUM)\ninter['p_full_sale'] = full_sale(inter, ['product_code'], week_num=WEEK_NUM)\n\ninter['i_daily_sale'] = inter['i_full_sale'] / inter['first_dat']\ninter['p_daily_sale'] = inter['p_full_sale'] / inter['first_dat']\ninter['i_daily_sale_ratio'] = inter['i_daily_sale'] / inter['p_daily_sale']\ninter['i_w_full_sale_ratio'] = inter['i_sale'] / inter['i_full_sale']\n\ninter['i_2w_full_sale_ratio'] = inter['i_2w_sale'] / inter['i_full_sale']\ninter['p_w_full_sale_ratio'] = inter['p_sale'] / inter['p_full_sale']\ninter['p_2w_full_sale_ratio'] = inter['p_2w_sale'] / inter['p_full_sale']\n\ninter['i_week_above_daily_sale'] = inter['i_sale'] / 7 - inter['i_daily_sale']\ninter['p_week_above_full_sale'] = inter['p_sale'] / 7 - inter['i_full_sale']\ninter['i_2w_week_above_daily_sale'] = inter['i_2w_sale'] / 14 - inter['i_daily_sale']\ninter['p_2w_week_above_daily_sale'] = inter['p_2w_sale'] / 14 - inter['p_daily_sale']","metadata":{"id":"OC8SIYK_Ca-g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"id":"334plAMZ5Ics","outputId":"d0d6230d-ba07-43d5-9640-ca8c97e60b7a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for f in tqdm(item_feats):\n    inter[f'{f}_full_sale'] = full_sale(inter, [f], week_num=WEEK_NUM)\n    f_first_date = inter.groupby(f)['t_dat'].min().reset_index(name=f'{f}_first_dat')\n    inter = inter.merge(f_first_date, on=f, how='left')\n    inter[f'{f}_daily_sale'] = inter[f'{f}_full_sale'] / (pd.to_datetime(current_dat) - pd.to_datetime(inter[f'{f}_first_dat'])).dt.days\n    inter[f'i_{f}_daily_sale_ratio'] = inter['i_daily_sale'] / inter[f'{f}_daily_sale']\n    inter[f'p_{f}_daily_sale_ratio'] = inter['p_daily_sale'] / inter[f'{f}_daily_sale']\n    del inter[f'{f}_full_sale'], inter[f'{f}_first_dat']\n    gc.collect()","metadata":{"id":"z4HdQYbZ5CC-","outputId":"daf9bf66-b53d-4504-bd05-b9a385f1fe49"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for f in item_feats + ['i_full_sale','p_full_sale']:\n    del inter[f]","metadata":{"id":"_fhlw8z4z5hD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inter['i_pop'] = popularity(inter, 'article_id', week_num=WEEK_NUM)\ninter['p_pop'] = popularity(inter, 'product_code', week_num=WEEK_NUM)","metadata":{"id":"ms4s0YUJznFU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inter = inter.loc[inter['week'] <= WEEK_NUM + 2]","metadata":{"id":"ip6AdvV0kS1y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inter.to_parquet(data_dir / \"processed/processed_inter.pqt\")","metadata":{"id":"huYRSLE91PJH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Merge Features\n","metadata":{"id":"jywhBCAGGGQS"}},{"cell_type":"code","source":"inter = pd.read_parquet(data_dir / \"processed/processed_inter.pqt\")\ninter = inter[inter['week'] <= WEEK_NUM + 2]","metadata":{"id":"YwbQ4jyD4MxX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#* embeddings from DSSM model\ndssm_user_embd = np.load(data_dir / \"external/dssm_user_embd.npy\", allow_pickle=True)\ndssm_item_embd = np.load(data_dir / \"external/dssm_item_embd.npy\", allow_pickle=True)\n# * embeddings from YouTubeDNN model\nyt_user_embd = np.load(data_dir / \"external/yt_user_embd.npy\", allow_pickle=True)\nyt_item_embd = np.load(data_dir / \"external/yt_item_embd.npy\", allow_pickle=True)\n# * embeddings from Word2Vector model\nw2v_user_embd = np.load(data_dir/'external'/'w2v_user_embd.npy', allow_pickle=True)\nw2v_item_embd = np.load(data_dir/'external'/'w2v_item_embd.npy', allow_pickle=True)","metadata":{"id":"-7YlHho_JR4F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in inter.columns:\n    inter[col] = np.nan_to_num(inter[col])","metadata":{"id":"IveB8V0VJGLm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in tqdm(range(WEEK_NUM)):\n    if i == 0 and not TEST:\n        continue\n    candidate = pd.read_parquet(data_dir/\"interim\"/VERSION_NAME/f\"week{i}_candidate.pqt\")\n    if i == 0:\n        chunk_size = int(candidate.shape[0] * 0.5)\n        for chunk,batch in enumerate(range(0, candidate.shape[0], chunk_size)):\n            sub_candidate = candidate.iloc[batch:batch+chunk_size-1]\n            # * merge features\n            sub_candidate = merge_week_data(data, inter, i, sub_candidate)\n            sub_candidate['article_id'] = sub_candidate['article_id'].astype(int)\n            sub_candidate['customer_id'] = sub_candidate['customer_id'].astype(int)\n            # * merge DSSM user and item embeddings\n            sub_candidate[\"dssm_similarity\"] = calc_embd_similarity(sub_candidate, dssm_user_embd, dssm_item_embd)\n            # * merge YouTubeDNN user and item embeddings\n            sub_candidate[\"yt_similarity\"] = calc_embd_similarity(sub_candidate, yt_user_embd, yt_item_embd)\n            # * merge Word2Vector user and item embeddings\n            sub_candidate[\"wv_similarity\"] = calc_embd_similarity(sub_candidate, w2v_user_embd, w2v_item_embd, sub=False)\n            print(f\"Chunk {chunk} done...\")\n            sub_candidate.to_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week{i}_candidate_{chunk}.pqt\")\n    else:\n        # * merge features\n        candidate = merge_week_data(data, inter, i, candidate)\n        print(candidate['week'].unique())\n        # * merge DSSM user and item embeddings\n        candidate[\"dssm_similarity\"] = calc_embd_similarity(candidate, dssm_user_embd, dssm_item_embd)\n        # * merge YouTubeDNN user and item embeddings\n        candidate[\"yt_similarity\"] = calc_embd_similarity(candidate, yt_user_embd, yt_item_embd)\n        candidate[\"wv_similarity\"] = calc_embd_similarity(candidate, w2v_user_embd, w2v_item_embd, sub=False)\n    candidate.to_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week{i}_candidate.pqt\")","metadata":{"id":"HXmOMDkO33sr","outputId":"2e393829-09ab-4fd6-9372-edc7dfeda088"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del dssm_user_embd, dssm_item_embd, yt_user_embd, yt_item_embd\ngc.collect()","metadata":{"id":"sWyKQBAt3x--","outputId":"fbf329ec-67aa-4074-f106-f90285bf1d58"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ranking\n","metadata":{"id":"7Zml-cxryoU_"}},{"cell_type":"code","source":"candidates = {}\nlabels = {}\nfor i in tqdm(range(1, WEEK_NUM)):\n    candidates[i] = pd.read_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week{i}_candidate.pqt\")\n    labels[i] = pd.read_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week{i}_label.pqt\")    ","metadata":{"id":"PsmE6rgjdtIr","outputId":"33501621-403f-4a60-f351-2a112cb1dfdb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feats = [\n    x\n    for x in candidates[1].columns\n    if x\n    not in [\n        \"label\",\n        \"sales_channel_id\",\n        \"t_dat\",\n        \"week\",\n    ]\n]\ncat_features = [\n    \"customer_id\",\n    \"article_id\",\n    \"product_code\",\n    \"FN\",\n    \"Active\",\n    \"club_member_status\",\n    \"fashion_news_frequency\",\n    \"age\",\n    \"product_type_no\",\n    \"product_group_name\",\n    \"graphical_appearance_no\",\n    \"colour_group_code\",\n    \"perceived_colour_value_id\",\n    \"perceived_colour_master_id\",\n\n    \"user_gender\",\n    \"article_gender\",\n    \"season_type\"\n]","metadata":{"id":"xWqYwCbsdtIr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# * Convert categorical featues as `CategoricalDtype`\ncate_dict = {}        \nfor feat in tqdm(cat_features):\n    if feat in data['user'].columns:\n        value_set = set(data['user'][feat].unique())\n    elif feat in data['item'].columns:\n        value_set = set(data['item'][feat].unique())\n    else:\n        value_set = set(data['inter'][feat].unique())\n    cate_dict[feat] = CategoricalDtype(categories=value_set)","metadata":{"id":"cweXHQMrJR4I","outputId":"4b89aacb-d149-4284-f0f4-1bd3cf472500"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_data = pd.concat([candidates[i] for i in range(1, WEEK_NUM)], ignore_index=True)","metadata":{"id":"G-fVjg3JT_OE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extra Features","metadata":{"id":"aLttJ3CtJ39a"}},{"cell_type":"code","source":"inter = data['inter']\ninter = inter[inter['t_dat']<'2020-08-19'] # * start date of the last valid week\ninter['week'] = (pd.to_datetime('2020-09-29') - pd.to_datetime(inter['t_dat'])).dt.days // 7\ninter = inter.merge(data['item'][[\"article_id\", \"product_code\"]], on=\"article_id\", how=\"left\")","metadata":{"id":"lcsSONupJ9zO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = inter.groupby('article_id').week.mean()\nfull_data['article_time_mean'] = full_data['article_id'].map(tmp)\n\ntmp = inter.groupby('customer_id').week.nth(-1)\nfull_data['customer_id_last_time'] = full_data['customer_id'].map(tmp)\n\ntmp = inter.groupby('customer_id').week.nth(0)\nfull_data['customer_id_first_time'] = full_data['customer_id'].map(tmp)\n\ntmp = inter.groupby('customer_id').week.mean()\nfull_data['customer_id_time_mean'] = full_data['customer_id'].map(tmp)\n\nfull_data['customer_id_gap'] = full_data['customer_id_first_time'] - full_data['customer_id_last_time']","metadata":{"id":"Z3vaGE2nJ-zT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feats += [\n    'article_time_mean', \n    'customer_id_last_time', \n    'customer_id_first_time', \n    'customer_id_time_mean',\n    'customer_id_gap'\n]","metadata":{"id":"RYEfzdHmczFn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del tmp\ngc.collect()","metadata":{"id":"psZK0985KCnl","outputId":"7d67a323-97ec-4435-ec61-3cb53bc634e5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train\n","metadata":{"id":"3RojtbK9GGQU"}},{"cell_type":"code","source":"for feat in tqdm(cat_features):\n    full_data[feat] = full_data[feat].astype(cate_dict[feat])","metadata":{"id":"4UeLfwqRc4iQ","outputId":"78f0b549-b400-4773-be89-c53272c85c37"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = full_data.loc[full_data['week']>1]\nvalid = full_data.loc[full_data['week']==1]\n\ndel full_data\ngc.collect()","metadata":{"id":"rJIW8LACUCmg","outputId":"8f48a53a-eedf-4e01-cc6c-4230bcac7111"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    \"objective\": \"lambdarank\",#\"lambdarank\",\n    \"boosting_type\": \"gbdt\",\n    \"metric\": \"map\",#\"map\",\n    \"max_depth\": 8,\n    \"num_leaves\": 128,\n    \"learning_rate\": 0.03,\n\n    \"verbose\": -1,\n    \"eval_at\": 12,\n}","metadata":{"id":"nTm9bz6vR6T4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_rank_model(train, valid, train_group, valid_group):\n\n    train_set = lgb.Dataset(\n        data=train[feats],\n        label=train[\"label\"],\n        group=train_group,\n        feature_name=feats,\n        categorical_feature=cat_features,\n        params=params,\n    )\n\n    valid_set = lgb.Dataset(\n        data=valid[feats],\n        label=valid[\"label\"],\n        group=valid_group,\n        feature_name=feats,\n        categorical_feature=cat_features,\n        params=params,\n    )\n\n    ranker = lgb.train(\n        params,\n        train_set,\n        num_boost_round=300,\n        valid_sets=[valid_set],\n        early_stopping_rounds=30,\n        verbose_eval=10,\n    )\n    ranker.save_model(\n        model_dir / f\"lgb_small_ranker.model\",\n        num_iteration=ranker.best_iteration,\n    )\n    return ranker","metadata":{"id":"G2wtl4vo9F8J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_binary_model(train, valid):\n\n    train_set = lgb.Dataset(\n        data=train[feats],\n        label=train[\"label\"],\n        feature_name=feats,\n        categorical_feature=cat_features,\n        params=params,\n    )\n\n    valid_set = lgb.Dataset(\n        data=valid[feats],\n        label=valid[\"label\"],\n        feature_name=feats,\n        categorical_feature=cat_features,\n        params=params,\n    )\n\n    ranker = lgb.train(\n        params,\n        train_set,\n        num_boost_round=300,\n        valid_sets=[valid_set],\n        early_stopping_rounds=30,\n        verbose_eval=10,\n    )\n    ranker.save_model(\n        model_dir / f\"lgb_small_binary.model\",\n        num_iteration=ranker.best_iteration,\n    )\n    return ranker","metadata":{"id":"01ZUO87Gg_-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del candidates\ngc.collect()","metadata":{"id":"vw2PmIZwPdFf","outputId":"596f7eee-2618-4311-eebd-b62c0fd9c635"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train positive rate:\", train.label.mean())","metadata":{"id":"F-wxWYRFPd9O","outputId":"7f5a0882-8a44-4533-abee-3ace8d20f93e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.sort_values(by=[\"week\", \"customer_id\"], ascending=True).reset_index(drop=True)\nvalid = valid.sort_values(by=[\"customer_id\"], ascending=True).reset_index(drop=True)","metadata":{"id":"O9JQdMFFPfov"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_group = train[[\"customer_id\", \"article_id\", \"week\"]]\ntrain_group = train_group.astype(\"int32\")  # * convert to int to avoid `0` in groupby count result\ntrain_group = (train_group.groupby([\"week\", \"customer_id\"]).size().values)","metadata":{"id":"aj70p1WDPgfQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_group = valid[[\"customer_id\", \"article_id\"]]\nvalid_group = valid_group.astype(\"int32\")  # * convert to int to avoid `0` in groupby count result\nvalid_group = valid_group.groupby([\"customer_id\"]).size().values","metadata":{"id":"lYOh4wBaPhaN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train[feats+['label']]\nvalid = valid[feats+['label']]","metadata":{"id":"Z1LrjrLKPiYA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"id":"lm8JNIuqPjMN","outputId":"cdfc2f52-28b9-47e7-dc54-0be7187fb16d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ranker = train_rank_model(train, valid, train_group, valid_group)\nranker = train_binary_model(train, valid)","metadata":{"id":"1FxwqE1cJR4L","outputId":"8d510452-faa1-473e-9240-38d6657fc4e5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference","metadata":{"id":"KbFdGrYRdtIt"}},{"cell_type":"code","source":"# ranker = lgb.Booster(model_file=model_dir / \"lgb_small_binary.model\")\nranker = lgb.Booster(model_file=model_dir / \"lgb_small_ranker.model\")","metadata":{"id":"hl3DPTaBGGQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importance = pd.DataFrame(\n    {\"feature\": feats, \"importance\": ranker.feature_importance()}\n).sort_values(by=\"importance\", ascending=False)\nplt.figure(figsize=(8, 22))\nsns.barplot(y=\"feature\", x=\"importance\", data=feat_importance)","metadata":{"id":"OG9AwSntGGQV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validate","metadata":{"id":"lLWs-Z5CGGQV"}},{"cell_type":"code","source":"val_candidates = valid.reset_index(drop=True)","metadata":{"id":"SSpgfI0mdtIv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(ranker, candidates, batch_size = 5_000_000):\n    probs = np.zeros(candidates.shape[0])\n    for batch in range(0, candidates.shape[0], batch_size):\n        outputs = ranker.predict(candidates.loc[batch : batch + batch_size - 1, feats])\n        probs[batch : batch + batch_size] = outputs\n    candidates[\"prob\"] = probs\n    pred_lgb = candidates[['customer_id','article_id','prob']]\n    pred_lgb = pred_lgb.sort_values(by=[\"customer_id\",\"prob\"], ascending=False).reset_index(drop=True)\n    pred_lgb.rename(columns={'article_id':'prediction'}, inplace=True)\n    pred_lgb = pred_lgb.drop_duplicates(['customer_id', 'prediction'], keep='first')\n    pred_lgb['customer_id'] = pred_lgb['customer_id'].astype(int)\n    pred_lgb = pred_lgb.groupby(\"customer_id\")[\"prediction\"].progress_apply(list).reset_index()\n    return pred_lgb","metadata":{"id":"pEhET3KZJR4N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = predict(ranker, val_candidates)","metadata":{"id":"Ifgoemo6JR4N","outputId":"f1accf63-1aeb-4c3e-bf0b-c0edf1daeb26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label = labels[1]\nlabel = pd.merge(label, pred, on=\"customer_id\", how=\"left\")","metadata":{"id":"G1Qh3k2aGGQW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"map_at_k(label[\"article_id\"], label[\"prediction\"], k=12)\n\n# 0.029813727108367518 ranker\n# 0.029791925075924913 binary","metadata":{"id":"XrFnQ2evGGQW","outputId":"505a0a6f-a7cf-4866-b305-abca1658aed5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 5_000_000\nprobs = np.zeros(val_candidates.shape[0])\nfor batch in range(0, val_candidates.shape[0], batch_size):\n    outputs = ranker.predict(val_candidates.loc[batch : batch + batch_size - 1, feats])\n    probs[batch : batch + batch_size] = outputs\nval_candidates[\"prob\"] = probs\npred_lgb = val_candidates[['customer_id','article_id','prob']]\npred_lgb = pred_lgb.sort_values(by=[\"customer_id\",\"prob\"], ascending=False).reset_index(drop=True)\npred_lgb.rename(columns={'article_id':'prediction'}, inplace=True)\npred_lgb = pred_lgb.drop_duplicates(['customer_id', 'prediction'], keep='first')\npred_lgb['customer_id'] = pred_lgb['customer_id'].astype(int)","metadata":{"id":"SEpG-KVUJR4O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_lgb.to_parquet(data_dir/\"processed\"/\"small_binary_valid.pqt\")","metadata":{"id":"UILjptYKJR4O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test","metadata":{"id":"_5_faqlKJR4R"}},{"cell_type":"code","source":"del candidates\ngc.collect()","metadata":{"id":"qFaPtoF7JR4R","outputId":"8817b209-c405-4874-9102-81fdbfab1ad4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred = []\nfor chunk in range(2):\n    print(f\"Chunk {chunk}\")\n    test_candidates = pd.read_parquet(data_dir/\"processed\"/VERSION_NAME/f\"week0_candidate_{chunk}.pqt\")\n    for feat in cat_features:\n        test_candidates[feat] = test_candidates[feat].astype(cate_dict[feat])\n\n    # * Extra Features ===================================\n\n    tmp = inter.groupby('article_id').week.mean()\n    test_candidates['article_time_mean'] = test_candidates['article_id'].map(tmp)\n\n    tmp = inter.groupby('customer_id').week.nth(-1)\n    test_candidates['customer_id_last_time'] = test_candidates['customer_id'].map(tmp)\n\n    tmp = inter.groupby('customer_id').week.nth(0)\n    test_candidates['customer_id_first_time'] = test_candidates['customer_id'].map(tmp)\n\n    tmp = inter.groupby('customer_id').week.mean()\n    test_candidates['customer_id_time_mean'] = test_candidates['customer_id'].map(tmp)\n\n    test_candidates['customer_id_gap'] = test_candidates['customer_id_first_time'] - test_candidates['customer_id_last_time']\n\n    gc.collect()\n    # * ==================================================\n    \n    batch_size = 5_000_000\n    probs = np.zeros(test_candidates.shape[0])\n    for batch in tqdm(range(0, test_candidates.shape[0], batch_size)):\n        outputs = ranker.predict(test_candidates.loc[batch : batch + batch_size - 1, feats])\n        probs[batch : batch + batch_size] = outputs\n    test_candidates[\"prob\"] = probs\n    pred_lgb = test_candidates[['customer_id','article_id','prob']]\n    pred_lgb = pred_lgb.sort_values(by=[\"customer_id\",\"prob\"], ascending=False).reset_index(drop=True)\n    pred_lgb.rename(columns={'article_id':'prediction'}, inplace=True)\n    pred_lgb = pred_lgb.drop_duplicates(['customer_id', 'prediction'], keep='first')\n    pred_lgb['customer_id'] = pred_lgb['customer_id'].astype(int)\n    test_pred.append(pred_lgb)\n    del test_candidates\n    gc.collect()","metadata":{"id":"GxTa-32qPMNh","outputId":"98bc03f6-32d4-4747-bec4-0a3024e5c09c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_lgb = pd.concat(test_pred, ignore_index=True)","metadata":{"id":"S1D0jEFqPzng"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred_lgb.to_parquet(data_dir/\"processed\"/\"small_binary_test.pqt\")\npred_lgb.to_parquet(data_dir/\"processed\"/\"small_rank_test.pqt\")","metadata":{"id":"ZpdiPY58JR4R"},"execution_count":null,"outputs":[]}]}